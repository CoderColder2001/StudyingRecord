# 机器学习

------
## 模块
### 残差连接

------
## 优化
通过最小化损失函数，调整神经网络模型的参数（如权重和偏差）的算法  

### Stochastic Gradient Descent 随机梯度下降
<b>在每次迭代中只随机选择一些数据点来计算梯度</b>   
*有时会使得收敛过程显得不规律；*  SGD的随机性和噪声数据导致算法的波动、不太稳定且收敛时间较长  

优点：
- 速度快
- 避免局部最小值。
- 在线学习：能够增量更新模型。适合在线学习，当新数据到来时需要更新模型

### adam optimizer
通过计算梯度的一阶和二阶矩估计，能够在训练过程中自适应调整学习率，从而提高训练效率和稳定性  


------
## 正则化
### batchNorm
常用于卷积层后  
*为激活层提供期望的分布，即先让分布标准化，然后再激活*  

### dropout
置于激活层后  
训练阶段，每一次迭代时随机选择一批单元暂时忽略（防止过拟合）  
BN和dropout一般不同时使用，如果一定要同时使用，可以将dropout放置于BN后面  

------
## 激活函数
### ReLU


### softmax
$e^{x_i}/\sum_{i=0}^{N-1}(e^{x_i})$  
将值（任意数列）**映射到概率分布**（总和为1）  
加入`T`（一般不大于2），$e^{x_i/T}/\sum_{i=0}^{N-1}(e^{x_i/T})$：`T`越大结果越平滑，分布更均匀    

<br>

------
## 架构
### 自编码器 AutoEncoder
将原始数据（高维、复杂）经 Encoder 后得到的向量（低维、经过处理）作为下游任务的输入，并希望还原出来的结果能够与原始数据尽可能的接近  

两个网络：Encoder和Decoder  

无监督的数据维度压缩和数据特征表达方法；本质上在 **寻找变化规律**

De-noising Auto-encoder：输入加噪声，但要求解码后与未加入噪声的输入尽可能接近  
（高维数据中存在信息冗余；希望能够从部分损坏的数据中得到有效的数据表示）  


---
### 扩散模型 Diffusion Model
生成式模型  
学习一个能从噪声中回复符合某种分布的原始数据的神经网络  
（学习“如何去噪”）本质是一个“预测噪声的网络”，然后逐步减去这些噪声  

由一个正向和一个反向的马尔可夫过程组成  

``` python
sample = random_sample
for t in range(T, 0, -1):
  extra_noise = random_sample if t > 1 else 0
  predicted_noise = trained_nn(x_tsub1, t) # x_t-1
  s1, s2, s3 = ddpm_scaling(t) # Denoising Diffusion Probabilistic Model
  sample = s1*(sample - s2*predicted_noise)+s3*extra_noise
```
Sampling  

------
## 聚类

### DBSCAN
基于密度的聚类算法，**通过密度连接的方式将点归为一类**，通过寻找被低密度区域分离的高密度区域，并将高密度区域作为一个聚类的“簇”   
将数据分为三类点：核心点（邻域点数大于阈值）、边界点（邻域点数小于阈值，但在某个核心点的邻域内）、噪声点  
关系：密度直达（在核心点的邻域内）、密度可达（密度直达的传播）、密度相连（存在另外一个点，对于它来说，两个点都密度可达）  

主要参数：
- $eps(\varepsilon)$：定义了点之间的最大距离，点在该距离内被认为是邻居。
- $min\_ samples$：定义了一个簇中所需的最小点数。如果一个点的邻居数大于或等于 $min\_ samples$，该点被视为核心点

特点：
- 能够发现任意形状的簇
- 能够识别噪声点（即无法归为任何簇的点）
- 无需预先指定簇的数量

局限性：
- 参数敏感
- 无法处理数据集中簇的密度差异较大的情况

空间开销：
- 距离矩阵：若显式计算和存储点之间的距离矩阵，空间开销为 $O(N^2)$。可以使用 KDTree 或 BallTree 进行邻域搜索以进行空间开销上的优化 
- 邻域列表：聚类过程中，DBSCAN 需要为每个点存储其 $eps$ 范围内的邻居点。这部分的空间开销与数据密度和 $eps$ 参数有关

<br>

### HDBSCAN
通过层次聚类的方式，**生成一个层次树结构，从中提取稳定的簇**，通过分析层次树的稳定性来自动确定最佳的簇数量   

核心距离（样本与第k个最近邻样本点的距离）：$core_k(x)=d(x,N^k(x))$  
样本间互达距离：$d_{mreach \_ k} (a, b)=max\{core_k(a),core_k(b),d(a,b)\}$  

（*互达距离使得密集点（核心距离较小的点）彼此保持相同的距离，至少是它的核心距离*）  
用互达距离来表示两个样本点之间的距离，使得密集区域的样本距离不受影响，而稀疏区域的样本点与其他样本点的距离被放大；增加了聚类算法对散点的鲁棒性   

聚类过程为 “分裂图” 的过程：
- 找到一个最小的边集合（最小生成树），从集合中删除任何边都会导致图分裂   
- 压缩聚类树（删除散点）： 
  - 自顶向下遍历树，看每个节点分裂时，如果左右有一个子结点的样本数大于最小族大小，用它替换当前节点；若都少于最小族大小，则删除子节点，不再继续分裂；若都大于最小族大小，正常向下分裂
- 提取聚类簇 
  - 定义$\lambda=\frac{1}{distance}$，$\lambda_{death}$表示当前节点被分裂成两个子结点时，对应断开边长度的倒数；$\lambda_{p}$表示样本点p因为分裂离开该节点时，对应断开边长度的倒数  
  - 每个节点的稳定性：$s_{cluster}=\sum_{p\in{cluster}}(\lambda_p-\lambda_{death})$（可见 $s_{cluster} <= 0$，且越大说明杂点越少）  
  - 自顶向下遍历树，找到稳定性大于其左右孩子节点的稳定性之和的节点作为聚类簇  

参数：
- $k$：当 $k$ 较大时，会使得核心距离变大、相互可达距离变大；这样会有更多样本点被分配到稀疏区域，即更多点将被视为散点 
- $min\_ cluster\_ size$：控制簇的最小大小（与 DBSCAN 的 $min\_ samples$ 类似，但在 HDBSCAN 中具有更多灵活性）
- $min\_ samples$（可选）：影响核心点定义和邻域大小，但主要通过 $min\_ cluster\_ size$ 控制聚类过程

空间开销：
- 最小生成树（MST）：HDBSCAN 基于构建的 MST 来识别簇，MST 的存储开销为 $O(NlogN)$
- 层次结构：HDBSCAN 构建一个层次树结构（dendrogram），其空间开销大致为 $O(N)$
- 核心点的计算：与 DBSCAN 类似，需要存储每个点的邻居信息

<br>

------
## 对比学习
基于无监督个体判别任务：每一个“实例”作为一个类，在特征空间中尽可能地分开  
<b>正样本和负样本在特征空间对比</b>   
负样本是学习有判别能力的特征表示的关键  

该实例作为正样本，数据集里所有其他实例作为负样本  
（用一个MemoryBank存储所有样本的特征，随着编码器的训练不断更新）  
NCEloss   

也可以采用同一minibatch中的所有其他样本作为负样本  
从而可以使用一个编码器作端到端的训练  

基于生成式任务：  
以通过编码器后得到未来时刻的特征输出作为正样本，任意选取输入得到的编码器的特征输出作为负样本  
增大“域”之间的互信息  
