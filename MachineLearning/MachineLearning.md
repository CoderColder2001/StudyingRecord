## 优化
### Stochastic Gradient Descent 随机梯度下降
<b>在每次迭代中只随机选择一些数据点来计算梯度</b>   
*有时会使得收敛过程显得不规律；*  SGD的随机性和噪声数据导致算法的波动、不太稳定且收敛时间较长  

优点：
- 速度快
- 避免局部最小值。
- 在线学习：能够增量更新模型。适合在线学习，当新数据到来时需要更新模型

### adam optimizer

### batchNorm

### dropout

### softmax
$e^{x_i}/\sum_{i=0}^{N-1}(e^{x_i})$  
将值（任意数列）映射到概率分布（总和为1）  
加入`T`（一般不大于2），$e^{x_i/T}/\sum_{i=0}^{N-1}(e^{x_i/T})$：`T`越大结果越平滑，分布更均匀    
