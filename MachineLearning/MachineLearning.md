## 优化
### Stochastic Gradient Descent 随机梯度下降
<b>在每次迭代中只随机选择一些数据点来计算梯度</b>   
*有时会使得收敛过程显得不规律；*  SGD的随机性和噪声数据导致算法的波动、不太稳定且收敛时间较长  

优点：
- 速度快
- 避免局部最小值。
- 在线学习：能够增量更新模型。适合在线学习，当新数据到来时需要更新模型

### adam optimizer

### batchNorm

### dropout